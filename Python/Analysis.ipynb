{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341f598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee8ddc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"insurance.db\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e241f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: combined_data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSELECT * FROM combined_data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: combined_data"
     ]
    }
   ],
   "source": [
    "cursor.execute('SELECT * FROM combined_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a51a4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM combined_data': no such table: combined_data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JDECKE46\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2664\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2664\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: combined_data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM combined_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m end = time.time()\n\u001b[32m      4\u001b[39m conn.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JDECKE46\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:708\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m         _is_table_name = pandas_sql.has_table(sql)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JDECKE46\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2728\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2719\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2726\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2727\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2729\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JDECKE46\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2676\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2673\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minner_exc\u001b[39;00m\n\u001b[32m   2675\u001b[39m ex = DatabaseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2676\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mDatabaseError\u001b[39m: Execution failed on sql 'SELECT * FROM combined_data': no such table: combined_data"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df=pd.read_sql(\"SELECT * FROM combined_data\",conn)\n",
    "end = time.time()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nQuery returned {df.shape[0]:,} rows x {df.shape[1]} columns in {end - start:.2f} seconds\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CUST_ORIG_DATE'] = pd.to_datetime(df['CUST_ORIG_DATE'],errors='coerce')\n",
    "df['ACCT_SUSPD_DATE'] = pd.to_datetime(df['ACCT_SUSPD_DATE'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logic applied here: if there is a termination date, then we know there is churn happening\n",
    "df['Churn'] = np.where(df['ACCT_SUSPD_DATE'].isna(),np.nan,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c58bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity = df['INDIVIDUAL_ID'].value_counts()\n",
    "duplicates_in_name = quantity[quantity > 1]\n",
    "print(\"Table of 'Individual_ID' duplicate column:\\n\", duplicates_in_name)\n",
    "\n",
    "duplicate_rows_count = df[df.duplicated(subset=[\"INDIVIDUAL_ID\"],keep=False)].shape[0]\n",
    "print(\"Number of dupliacted customers: \",duplicate_rows_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804fb3f",
   "metadata": {},
   "source": [
    "The cell above shows that there are no duplicates in the individual_id column which is good to set this as the true value for all of our modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity = df['ADDRESS_ID'].value_counts()\n",
    "duplicates_in_name = quantity[quantity > 1]\n",
    "print(\"Table of 'ADDRESS_ID' duplicate column:\\n\", duplicates_in_name)\n",
    "\n",
    "duplicate_rows_count = df[df.duplicated(subset=[\"ADDRESS_ID\"],keep=False)].shape[0]\n",
    "print(\"Number of duplicated addresses: \",duplicate_rows_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401bc49",
   "metadata": {},
   "source": [
    "As we can see, there are many duplicated addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': df.isna().sum(),\n",
    "    'Percentage': (df.isna().sum() / len(df))*100\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291db4f",
   "metadata": {},
   "source": [
    "Notes on missing data:\n",
    "-The AGE_IN_YEARS would be expected to be 0 since it is from the customer table (the primary table) but there are just nulls in their anyways despiste a COALESCE from the autoinsurance_churn file\n",
    "-The ACCT_SUSPD_DATE is lean since the termination file is lean on data but that predictor will not be used in modeling anyways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be my helper function for parsing through and determining the median for each range of numbers that I was given\n",
    "def parse_home_value(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    val = str(val).strip()\n",
    "    if val.upper() == \"N/A\":\n",
    "        return np.nan\n",
    "    if \" - \" in val:\n",
    "        lo, hi = val.split(\" - \")\n",
    "        return (float(lo) + float(hi)) / 2\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return np.nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's get into the feature engineering part\n",
    "df = df.dropna(subset=['CURR_ANN_AMT']) # Drop the rows without curr_ann_amt\n",
    "df = df.dropna(subset=['INDIVIDUAL_ID']) # Drop the rows without individual_id's (Only 1 row for some reason)\n",
    "\n",
    "# Here will be my block of code for correcting out the na's and range values given for home_market_value\n",
    "df[\"HOME_MARKET_VALUE\"] = df[\"HOME_MARKET_VALUE\"].apply(parse_home_value)\n",
    "overall_home_median = df[\"HOME_MARKET_VALUE\"].median()\n",
    "print(\"Overall Home Median :\", type(overall_home_median))\n",
    "df[\"HOME_MARKET_VALUE\"] = df[\"HOME_MARKET_VALUE\"].fillna(overall_home_median)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nQuery returned {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac10b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': df.isna().sum(),\n",
    "    'Percentage': (df.isna().sum() / len(df))*100\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a32c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target\n",
    "y = df['CURR_ANN_AMT']\n",
    "\n",
    "# choose predictors\n",
    "features = [\n",
    "    'age_in_years','income','DAYS_TENURE',\n",
    "    'LENGTH_OF_RESIDENCE','HOME_MARKET_VALUE',\n",
    "    'Churn','HAS_CHILDREN','marital_status',\n",
    "    'HOME_OWNER','COLLEGE_DEGREE','GOOD_CREDIT',\n",
    "    'STATE','COUNTY'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "Columns_Before_One_Hot_Encoding = X.shape[1]\n",
    "\n",
    "# Function to obtain how many of each county is in the dataset\n",
    "uniqueCounties = X['COUNTY'].value_counts()\n",
    "\n",
    "## I want to see the max columns shown in the output channel\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "## This get_dummies function one hot encodes the county column and completely drops the state column since Texas is the only option\n",
    "X = pd.get_dummies(X,drop_first=True,dtype=int)\n",
    "\n",
    "# This yields me 13 columns - I looked through and the county that was removed was Collin so if a row is all 0's, then it will be assumed to be Collin county\n",
    "Columns_After_One_Hot_Encoding = X.shape[1]\n",
    "New_Columns = Columns_After_One_Hot_Encoding - Columns_Before_One_Hot_Encoding\n",
    "print(f\"Number of Columns Added: \",New_Columns)\n",
    "print(f\"Chart of County Counts: \",uniqueCounties)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83107e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': X.isna().sum(),\n",
    "    'Percentage': (X.isna().sum() / len(df))*100\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': X.isna().sum(),\n",
    "    'Percentage': (X.isna().sum() / len(df))*100\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7ee9d",
   "metadata": {},
   "source": [
    "For missing values, let's implement a \"was_missing\" feature so the model can learn if missingness correlates with premiums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81585e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the missing and n/a values for 'Has_Children', 'Home_Owner', 'College_Degree', and 'Good_Credit': since the missing values only represent 7% of the dataset, let's put in a \"Is_missing\" category to see if missingness correlates with premiums.\n",
    "for col in ['HAS_CHILDREN','HOME_OWNER','COLLEGE_DEGREE','GOOD_CREDIT']:\n",
    "    X[col + '_missing'] = X[col].isna().astype(int)\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': X.isna().sum(),\n",
    "    'Percentage': (X.isna().sum() / len(df))*100\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55939b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For age, income, and length of residence: fill in the missing values with the median values\n",
    "for col in ['age_in_years','income','LENGTH_OF_RESIDENCE']:\n",
    "    X[col + '_missing'] = X[col].isna().astype(int)\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': X.isna().sum(),\n",
    "    'Percentage': (X.isna().sum() / len(df))*100\n",
    "})\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28781f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "glm_gamma = sm.GLM(\n",
    "    y_train,\n",
    "    X_train_const,\n",
    "    family=sm.families.Gamma(sm.families.links.log())\n",
    ")\n",
    "\n",
    "results=glm_gamma.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_const = sm.add_constant(X_test)\n",
    "y_pred = results.predict(X_test_const)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(\"RMSE:\",rmse)\n",
    "print(\"MAE:\",mae)\n",
    "\n",
    "pd.set_option(\"display.float_format\",\"{:.2f}\".format)\n",
    "y_summary_stats = y.describe()\n",
    "print(y_summary_stats)\n",
    "\n",
    "mean_premium = y.mean()\n",
    "rel_mae = mae / mean_premium * 100\n",
    "rel_rmse = rmse / mean_premium * 100\n",
    "print(f\"Relative MAE: {rel_mae:.3f}%\")\n",
    "print(f\"Relative RMSE: {rel_rmse:.3f}%\")\n",
    "\n",
    "baseline_pred = np.full_like(y_test,fill_value=mean_premium)\n",
    "baseline_mae = mean_absolute_error(y_test,baseline_pred)\n",
    "print(\"Baseline MAE:\",baseline_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
